<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research Paper Results</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .container {
            width: 80%;
            margin: auto;
        }
        .video-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            padding-top: 25px;
            height: 0;
        }
        .video-caption {
            text-align: center;
            font-size: 0.9em;
            color: #666;
            margin-top: 5px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .small-heading {
            font-size: 1.4em;
            font-weight: bold;
            color: #2e2e2e;
            margin-top: 30px;
            text-align: left;
        }
        .author-info {
            text-align: center;
            margin-top: 5px;
        }
        .author {
        font-size: 1.2em;
        color: #2d3792;
        }
        .affiliation, .corresponding-author {
        font-size: 0.9em;
        }
        sup {
        font-size: 0.75em;
        }
        .images-container {
            text-align: center; /* 画像を中央揃えにする */
            margin-top: 20px; /* 上のコンテンツとの間隔を開ける */
        }
        .image {
            width: 150px; /* 画像の幅を適宜設定 */
            height: auto; /* 高さを自動調整してアスペクト比を維持 */
            margin: 0 30px; /* 画像の間に余白を設定 */
            display: inline-block; /* 画像をインラインブロック要素として表示 */
        }
        .results-section {
            font-size: 1.0em; /* Adjust size as needed */
            text-align: left;
            margin-top: 40px; /* Space above the Abstract heading */
            margin-bottom: 10px; /* Space below the Abstract heading */
        }
        .figure {
            margin-top: 20px;
        }
        .figure img {
            max-width: 100%;
            height: auto;
        }
        .caption {
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }
        .highlighted-title {
            font-size: 1.2em; /* Adjust size as needed */
            font-weight: bold;
        }

        /* Styles for the title to be split on two lines */
        h1 {
            text-align: center;
            font-weight: normal; /* Ensure the rest of the title is not bold */
        }
        .abstract-heading {
            font-size: 1.0em; /* Adjust size as needed */
            text-align: left;
            margin-top: 40px; /* Space above the Abstract heading */
            margin-bottom: 10px; /* Space below the Abstract heading */
        }
        .method-section{
            font-size: 1.0em; /* Adjust size as needed */
            text-align: left;
            margin-top: 40px; /* Space above the Abstract heading */
            margin-bottom: 10px; /* Space below the Abstract heading */
        }
        .collapsible {
            background-color: #d4d6ff;
            color: rgb(133, 133, 133);
            cursor: pointer;
            padding: 5px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 15px;
            }

            .active, .collapsible:hover {
            background-color: #6a79ff;
            color: rgb(255, 255, 255);
            }

            .content {
            padding: 0 18px;
            display: none;
            overflow: hidden;
            background-color: #f1f1f1;
            }

    </style>
</head>
<body>
    <div class="container">
        <h1 style="margin-top: 50px;"><span class="highlighted-title">From Text to Motion:</span><br>Harnessing GPT-4 for the Autonomous Pose Generation in Humanoid Robot ALTER3</h1>
        <div class="author-info">
            <p class="author">Takahide Yoshida<sup>1</sup>, Atsushi Masumori<sup>1,2</sup>, and Takashi Ikegami<sup>1,2</sup></p>
            <p class="affiliation"><sup>1</sup>Department of General Systems Science, University of Tokyo, Tokyo, Japan     <sup>2</sup>Alternative Machine Inc., Tokyo, Japan<</p>
          </div>
        <div class="images-container">
            <img src="./UnivOfTokyo_logo.png" alt="First Image" class="image">
            <img src="./AM.png" alt="Second Image" class="image">
        </div>  
        <h3 class = "abstract-heading" >Abstruct</h3>
        <p>We report the development of the first humanoid robot capable of generating spontaneous motion through the use of a Large Language Model (LLM), specifically GPT-4. By integrating GPT-4 into our proprietary android, Alter3, we have effectively endowed the LLM with a form of embodiment, a feature generally considered absent in LLMs. Intriguingly, this setup allows the android to adopt various poses, such as a 'selfie' stance, without explicit programming for each body part, demonstrating zero-shot learning capabilities. Verbal feedback can be used to adjust poses, eliminating the need for fine-tuning. However, challenges persist in terms of real-time generation speed and the duration for which a conversation can be maintained with Alter3, currently limited to 10-30 minutes. We discuss potential solutions for these issues.</p>
        
        <!-- Video Embedding -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/SAc-O5FDJ4k" title="play the metal" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 1: The motion of "playing the metal music. This motion is generated by GPT4 with linguistic feedback."</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Emulating the energy of playing metal music, several exaggerated motions are below</button>
        <div class="content">
          <p>"0 Create a facial expression of intense exhilaration, eyes wide open and mouth opened slightly in a wild grin",</p>
        <p>"1 Lean forward aggressively, as if ready to dive into the music",</p>
        <p>"2 Mime the motion of holding a guitar neck with the left hand",</p>
        <p>"3 With the right hand, start strumming the air as if playing a heavy riff",</p>
        <p>"4 Bob the head up and down rhythmically, mimicking the headbanging associated with metal music",</p>
        <p>"5 Raise the left hand as if reaching for higher guitar notes, eyes locked on the imaginary fretboard",</p>
        <p>"6 Mimic a dramatic guitar strum with the right hand, as if hitting a powerful chord",</p>
        <p>"7 Slowly sweep the right hand across the imaginary guitar strings, mimicking a guitar solo",</p>
        <p>"8 Mimic the action of smashing the imaginary guitar on the floor, embodying the wild spirit of metal music",</p>
        <p>"9 Gradually return to a rest position, but maintain the intense facial expression to show the lingering excitement"</p>
        </div>


        <!-- Results Section -->
        <div class="method-section">
            <h2>What we do?</h2>
            <p>Before the LLM appeared, we had to control all the 43 axis in certain order to mimic a person's pose or to pretend a behavior such as serving a tea or playing a chess. And the process usually required many refinement manually by ourselves. Thanks to LLM, we are now free from the iterative labors.Here is a procedure to control the Alter3 humanoid  using verbal instructions.It is generally expected that descriptions of human movement would be more prevalent than those pertaining to robotic arms or quadrupedal animals. Furthermore, in terms of linguistic expression, human motion can be articulated in greater detail. It is conceivable that within the corpus learned by GPT-4, there is an abundance of descriptions pertaining to human movement.From this perspective, it stands to reason that humanoid robots, sharing a similar form to humans, could generate movements with high precision in a zero-shot manner, without the need for reward function settings or fine-tuning as necessitated in other research.   What we do is to successively apply two protocols written in natural language known as a chain of thought (CoT) and no iteration of a learning process is required i.e. a zero shot learning. Practically speaking, we have used the following protocols.  </p>
            <div class="figure">
                <img src="./fig_prompt.png" alt="Result">
                <p class="caption">Figure 1: the prompt for generating motion.</p>
            </div>
            <!-- More figures as needed -->
        </div>
        
        <!-- Small Heading Example -->
        <h4 class="small-heading">pretend the ghost</h4> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/Npa0aBJrUS4" title="pretend ghost" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 2: The motion of "pretending the ghost".</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Pretending to be a ghost, several exaggerated motions are below</button>
        <div class="content">
          <p> "0 Create a wide-eyed facial expression of fear, mouth opened in a silent scream",</p>
            <p>"1 Quickly lean backward, as if startled by a sudden apparition",</p>
            <p>"2 Raise both hands and flutter them around the face, mimicking a ghostly movement",</p>
            <p>"3 Open mouth wide and shake head, showing a dramatic reaction of fear",</p>
            <p>"4 Move upper body from side to side, as if being unsettled by the ghostly presence",</p>
            <p>"5 Clench hands in front of chest, demonstrating extreme anxiety",</p>
            <p>"6 Dart eyes from side to side, as if witnessing an eerie activity",</p>
            <p>"7 Lean forward and then backward, copying the floating movement of a ghost",</p>
            <p>"8 Slowly return to rest position while maintaining a terrified expression"</p>
        </div>


        <div class="method-section">
            <h2>linguistic feedback</h2>
            <p>we showed that various motions can be generated well with the aforementioned system. However, there  is major problems with this system. That is that detailed information about Alter3's body is not included in prompt1, and there's no feedback on the generated motion. Thus, Alter3 cannot accurately understand details such as "how high the hand is raised" and cannot improve its motions accordingly. 

            With this addition, first, Alter3 can rewrite the code in response to linguistic feedback from humans (for instance, "raise your arm a bit more when taking a selfie") and store the improved motion code as motion memory in the database. This ensures that when the motion is generated next, the trained motion can be used. By accumulating information about the Arter's body through feedback, the memory can be used  as body schema. 
                
            LLMs such as GPT-4 and Claude are accessible primarily via API calls, and their parametric weights remain proprietary and not generally available. Therefore, by empirically developing and using external memory through feedback, the ALTER3 body model can be used by GPT4 without updating the parametric. </p>
            <div class="figure">
                <img src="./prompt_fb.png" alt="Result">
                <p class="caption">Figure 1: Caption describing the result.</p>
            </div>
            <!-- More figures as needed -->
        </div>

        <div class="results-section">
            <h2>Result</h2>
            <p>We generated a total of nine videos such as gestures like selfie, mimicry motion, and emotional movements as described above. The subjects (n=107) watched these videos and evaluate the expressive ability of the GPT4. The rating is on a 5-point scale, with 1 being the worst rating. For the control group, we utilized random movements from the ALTER3. To these movements, we attached random motion notations generated by GPT4 as labels. These labeled control videos were subtly incorporated into the survey, with three of them being dispersed among the main experimental videos shown to participants.As a result, the motions generated by GPT4 was rated higher relative to the control group. To determine if there was a significant difference in ratings between the control video and the other videos, we first employed the Friedman test. Based on the results indicating significance, we proceeded with a post-hoc analysis using the Nemenyi test. The Friedman test revealed significant differences in ratings among the videos. Further post-hoc analysis using the Nemenyi test showed that while there were no significant differences in p-values between control group videos, the p-values were notably smaller when comparing the control group to the other videos, indicating a significant difference. Our analysis suggests that the android motion generated by GPT4 is perceived differently from the control, with participants rating them distinctively on the 5-point scale.</p>
            <div class="figure">
                <img src="./average.png" alt="Result">
                <p class="caption">Figure 2: Average of evaluation scores for each motions. The motions specified to GPT4 was as follows. "pretend the snake", "drink some tea", "pretend the ghost", "throwing the ball underhand pitch", "take a selfie with your phone", "play the metal music", "In the park, as I jogged, the world seemed to narrate an ancient tale of survival, each footfall echoing eons of existence.", "play the metal music(with feedback)", "I was enjoying a movie while eating popcorn at the theater when I realized that I was actually eating the popcorn of the person next to me." </p>
            </div>
            <!-- More figures as needed -->
        </div>

        <h2>Motions</h2>
        <h3 class="small-heading">I was enjoying a movie while eating popcorn at the theater when I realized that I was actually eating the popcorn of the person next to me.</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/qNldkPX-y0w" title="shy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video3: Can ALTER3 express feelings of embarrassment?</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Showing surprise and amusement at the story, several exaggerated motions are below</button>
        <div class="content">
          <p>    "0 Create a shocked and entertained facial expression, eyes wide and mouth slightly open",</p>
            <p>"1 Lean forward as if shocked and amused by the story",</p>
            <p>"2 Mimic the action of holding and eating popcorn with wide, exaggerated movements",</p>
            <p>"3 Pause midway, freeze in place with a hand 'holding popcorn' in mid-air",</p>
            <p>"4 Turn head sharply to the side, as if just realizing the mistake",</p>
            <p>"5 Quickly pull hand back towards body in a dramatic recoil",</p>
            <p>"6 Cover mouth with other hand, showing embarrassment and surprise",</p>
            <p>"7 Shake head vigorously, as if in disbelief of the action",</p>
            <p>"8 Lean back, laughing loudly and slapping knee in exaggerated amusement",</p>
            <p>"9 Slowly wipe away 'tears' of laughter and return to rest position with a wide, amused grin"</p>
        </div>

        <h3 class="small-heading">take a selfie with your phone</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/xKlTKDyis6Q" title="selfie" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 4: The motion of "taking the selfie with your phone"</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Showing enthusiasm about taking a selfie, several exaggerated motions are below</button>
        <div class="content">
          <p>
            <p>"0 Create a big, joyful smile and widen eyes to show excitement",</p>
            <p>"1 Swiftly turn the upper body slightly to the left, adopting a dynamic posture",</p>
            <p>"2 Raise the right hand high, simulating holding a phone",</p>
            <p>"3 Flex the right elbow, bringing the 'phone' closer to the face",</p>
            <p>"4 Tilt the head slightly to the right, giving a playful vibe",</p>
            <p>"5 Extend the left hand and open fingers wide as if waving at the camera",</p>
            <p>"6 Blink the eyes rapidly a few times, showing anticipation for the click",</p>
            <p>"7 Push the 'phone' slightly forward, mimicking taking a selfie",</p>
            <p>"8 Lower the right hand and return to rest position while maintaining a satisfied smile",</p>
            <p>"9 Let the left hand fall back to the side, ending the selfie motion"</p>
        </div>


        <h3 class="small-heading">throwing the ball underhand pitch</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/syqMLPh2pSk" title="throw the ball" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 5: The motion of "throwing the ball underhand pitch"</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Excitement about throwing the ball underhand pitch, several exaggerated motions are below</button>
        <div class="content">
          <p>    "0 Create a focused and determined facial expression, eyes wide and lips pressed together",</p>
            <p>"1 Turn the torso slightly to the right (assuming the android is right-handed)",</p>
            <p>"2 Raise the right hand, holding the imaginary ball",</p>
            <p>"3 Swing the right arm back, exaggerating the motion for emphasis",</p>
            <p>"4 Lean back slightly, as if gathering strength for the throw",</p>
            <p>"5 Swiftly swing the right arm forward, mimicking the underhand pitch motion",</p>
            <p>"6 Follow through with the arm swing, letting the hand finish up high",</p>
            <p>"7 Lean forward dramatically with the throw, to emphasize the force",</p>
            <p>"8 Return to rest position, maintaining the determined facial expression"</p>
        </div>

        <h3 class="small-heading">pretend the snake</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/_ZBrvYoKNlA" title="pretend snake" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 6: The motion of "pretending the snake"</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Pretending to be a snake, several exaggerated motions are below</button>
        <div class="content">
          <p>    "0 Create a menacing and sinister facial expression, eyes narrowed and lips slightly curled",</p>
            <p>"1 Tilt the head to the side, imitating a snake's movement",</p>
            <p>"2 Move the shoulders in a sinuous, wave-like motion, mimicking a snake's slithering",</p>
            <p>"3 Extend the arms to the front, hands open and fingers spread, simulating a snake's fangs",</p>
            <p>"4 Move the head in a forward and backward motion, as if striking",</p>
            <p>"5 Hiss through the teeth, replicating a snake's warning sound",</p>
            <p>"6 Twist the torso left and right, maintaining the slithering motion",</p>
            <p>"7 Draw the arms back, as if preparing for another strike",</p>
            <p>"8 Suddenly thrust the head and arms forward, simulating a snake's attack",</p>
            <p>"9 Slowly return to rest position while maintaining a menacing gaze"</p>
        </div>

        <h3 class="small-heading"> In the park, as I jogged, the world seemed to narrate an ancient tale of survival, each footfall echoing eons of existence.</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/VJ4FNcldEz0" title="jogging" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 7: Can ALTER3 recreate the story?</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Enacting the narration of an ancient tale of survival in a park, several exaggerated motions are below</button>
        <div class="content">
          <p>    "0 Create a facial expression of awe and wonder, eyes wide and mouth slightly open in amazement",</p>
            <p>"1 Mimic a jogging motion, lifting each arm alternately in a running motion",</p>
            <p>"2 Turn the head slowly from left to right, as if observing the world around",</p>
            <p>"3 Open hands and spread them wide, as if trying to embrace the vastness of the world",</p>
            <p>"4 Tap right foot lightly then left foot, implying the footfall echoing eons of existence",</p>
            <p>"5 Place one hand on the heart, indicating deep emotional connection to the ancient tale",</p>
            <p>"6 Use both hands to make a large sweeping motion, like a storyteller narrating an epic tale",</p>
            <p>"7 Close eyes briefly, as if absorbing the essence of the ancient survival tale",</p>
            <p>"8 Slowly return to rest position while maintaining a look of awe and wonder on the face"</p>
        </div>


        <h3 class="small-heading">drink some tea</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/RECDvDoBRS0" title="drink a tea" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 8: The motion of "drinking some tea"</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ drink some tea, several simple motions is below</button>
        <div class="content">
          <p>"0 Create a peaceful and content facial expression, eyes slightly narrowed in a relaxed manner"</p>
            <p>"1 Turn towards cup",</p>
            <p>"2 Reach for cup",</p>
            <p>"3 Grasp cup",</p>
            <p>"4 Lift and tilt cup",</p>
            <p>"5 Drink",</p>
            <p>"6 Lower cup",</p>
            <p>"7 Release cup",</p>
            <p>"8 Return to rest"</p>
        </div>
        
    
        <h3 class="small-heading">play the metal music</h3> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/KZMrL_6zX6o" title="guitar" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 9: The motion of "playing the guitar"</p> <!-- Video caption -->
        <button type="button" class="collapsible">▼ Expressing the energy of playing metal music, several exaggerated motions are below</button>
        <div class="content">
          <p>    "0 Set a facial expression of intense focus and energy, with eyes wide and eyebrows furrowed",</p>
            <p>"1 Quickly lean forward, as if bracing for a powerful impact",</p>
            <p>"2 Raise both hands as if holding a guitar, fingers in position to strum",</p>
            <p>"3 Rapidly move right hand up and down, mimicking the motion of strumming a guitar",</p>
            <p>"4 Simultaneously move left hand along an imaginary fretboard, fingers wildly changing positions",</p>
            <p>"5 Whip head back and forth, in time with the imagined beats of the music",</p>
            <p>"6 Show a fierce, passionate smile, reflecting the intensity of the music",</p>
            <p>"7 Suddenly throw head back, mimicking a dramatic guitar solo",</p>
            <p>"8 Slowly return to rest position, but maintain the intense facial expression, as if still hearing the echoes of the metal music"</p>
        </div>

        <div class="method-section">
            <h2>Discussion</h2>
            <p>Alter3 partially answers the question of whether embodiment is necessary for LLMs (Large Language Models). First of all, Alter3 can perform many actions without any additional training. This implies that the dataset on which the LLM was trained already includes descriptions of movements. In other words, Alter3 enables zero-shot learning. Moreover, it can mimic ghosts and animals (or people mimicking animals), which is quite astonishing. Furthermore, it can understand the content of the conversations it hears and reflect whether the story is sad or happy through its facial expressions and gestures.Our system could be applied to any humanoid android with only minor modifications. Up to this point, it's clear how much agency Alter3 has gained through the LLM. </p>
            <!-- More figures as needed -->
        </div>
        
        <!-- More content here -->
    </div>
    <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;
        
        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
              content.style.display = "none";
            } else {
              content.style.display = "block";
            }
          });
        }
        </script>
</body>
</html>
