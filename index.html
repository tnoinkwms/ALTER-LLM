<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research Paper Results</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .container {
            width: 80%;
            margin: auto;
        }
        .video-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            padding-top: 25px;
            height: 0;
        }
        .video-caption {
            text-align: center;
            font-size: 0.9em;
            color: #666;
            margin-top: 5px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .small-heading {
            font-size: 2.0em;
            font-weight: normal;
            color: #2e2e2e;
            margin-top: 30px;
            text-align: center;
        }
        .author {
            font-size: 1.5em;
            text-align: center;
            margin-top: 5px;
            color: #2ec7ff;
        }

        .results-section {
            margin-top: 20px;
        }
        .figure {
            margin-top: 20px;
        }
        .figure img {
            max-width: 100%;
            height: auto;
        }
        .caption {
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }
        h1 {
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>From Text to Motion: Harnessing GPT-4 for the Autonomous Pose Generation in Humanoid Robot ALTER3</h1>
        <p class="author">Takahide Yoshida, Atsushi Masumori and Takashi Ikegami</p> 
        <p>We report the development of the first humanoid robot capable of generating spontaneous motion through the use of a Large Language Model (LLM), specifically GPT-4. By integrating GPT-4 into our proprietary android, Alter3, we have effectively endowed the LLM with a form of embodiment, a feature generally considered absent in LLMs. Intriguingly, this setup allows the android to adopt various poses, such as a 'selfie' stance, without explicit programming for each body part, demonstrating zero-shot learning capabilities. Verbal feedback can be used to adjust poses, eliminating the need for fine-tuning. However, challenges persist in terms of real-time generation speed and the duration for which a conversation can be maintained with Alter3, currently limited to 10-30 minutes. We discuss potential solutions for these issues.</p>
        
        <!-- Video Embedding -->
        <h2 class="small-heading">Experimental Video Analysis</h2> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/SAc-O5FDJ4k" title="play the metal" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 1: Humanoid robot ALTER3 performing tasks with GPT-4 integration</p> <!-- Video caption -->

        <!-- Small Heading Example -->
        <h2 class="small-heading">Experimental Video Analysis</h2> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/Npa0aBJrUS4" title="pretend ghost" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 1: Humanoid robot ALTER3 performing tasks with GPT-4 integration</p> <!-- Video caption -->

        <h2 class="small-heading">Experimental Video Analysis</h2> <!-- Small heading for a new section -->
        <div class="video-container">
            <iframe width="423" height="752" src="https://www.youtube.com/embed/qNldkPX-y0w" title="shy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
        <p class="video-caption">Video 1: Humanoid robot ALTER3 performing tasks with GPT-4 integration</p> <!-- Video caption -->

        <!-- Results Section -->
        <div class="results-section">
            <h2>Results</h2>
            <p>We generated a total of nine videos such as gestures like selfie, mimicry motion, and emotional movements as described above. The subjects (n=107) watched these videos and evaluate the expressive ability of the GPT4. The rating is on a 5-point scale, with 1 being the worst rating. For the control group, we utilized random movements from the ALTER3. To these movements, we attached random motion notations generated by GPT4 as labels. These labeled control videos were subtly incorporated into the survey, with three of them being dispersed among the main experimental videos shown to participants.As a result, the motions generated by GPT4 was rated higher relative to the control group. </p>
            <div class="figure">
                <img src="./average.png" alt="Result">
                <p class="caption">Figure 1: Caption describing the result.</p>
            </div>
            <!-- More figures as needed -->
        </div>
        
        <!-- More content here -->
    </div>
</body>
</html>
